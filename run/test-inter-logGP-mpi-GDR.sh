#!/bin/bash
#SBATCH -N 2 # request 1 nodes
##SBATCH --nodelist=node01,node02
#SBATCH --output=./stdout/GDR/netgauge_run_%j.stdout    # standard output will be redirected to this file, where the % is replaced with the job allocation number.
#SBATCH -J "netgauge_run"    # this is your jobâ€™s name
#SBATCH --gpus-per-node=1

# ---[ Script Setup ]---

set -e

# module load mpich

# module load mpich/3.4.2-nvidiahpc-21.9-0

# MPI_HOME="/opt/apps/mpi/mpich-3.4.2_nvidiahpc-21.9-0"
# export MPI_HOME
# export PATH="${MPI_HOME}/include:$PATH"
# export LD_LIBRARY_PATH="${MPI_HOME}/lib:$LD_LIBRARY_PATH"

source /home/liuyao/sbatch_sh/.mpich_ucx_gdr

source /home/liuyao/sbatch_sh/.nvccrc

# for the shared library generated by myself

export LD_LIBRARY_PATH=/home/liuyao/software/Netgauge_gdr/libcuda:$LD_LIBRARY_PATH

ldd /home/liuyao/software/Netgauge_gdr/libcuda/libMyCudaCode.so

echo "UCX_NET_DEVICES=mlx5_0:1 mpirun -n 2 /home/liuyao/software/Netgauge_gdr/netgauge D D -m mpi -x loggp -o ng_logGP_internode"

dool --time --mem --cpu --net -N ib0,ens786f1,lo,total 1 > /home/liuyao/sbatch_sh/netgauge_gdr/run/output/GDR/CPU.csv  &
        nvidia-smi --query-gpu=name,timestamp,uuid,utilization.gpu,memory.total,utilization.memory,power.draw --format=csv -l 1 > /home/liuyao/sbatch_sh/netgauge_gdr/run/output/GDR/GPU.csv &
        sh rtop.sh -d ib0 > /home/liuyao/sbatch_sh/netgauge_gdr/run/output/GDR/RTOP.csv  &

mpirun -n 2 hostname

# mpirun -n 2 -host {node01,node02} /home/liuyao/software/Netgauge_default/netgauge -m mpi -x loggp -o ng_logGP_internode

### 'en2'(tcp), 'ens786f0'(tcp), 'ib0'(tcp), 'lo'(tcp), 'mlx5_0:1'(ib), 'mlx5_1:1'(ib), 'mlx5_2:1'(ib)

UCX_NET_DEVICES=mlx5_0:1 mpirun -n 2 /home/liuyao/software/Netgauge_gdr/netgauge -m mpi -x loggp -o ng_logGP_internode D D

# mpirun -n 2 /home/liuyao/software/Netgauge_default/netgauge -m ib -x overlap -o ng_logGP_intrano



