	linux-vdso.so.1 (0x00007fff93be8000)
	libnccl.so.2 => /opt/nvidia/hpc_sdk/Linux_x86_64/21.9/comm_libs//nccl/lib/libnccl.so.2 (0x00001520c3b54000)
	librt.so.1 => /lib64/librt.so.1 (0x00001520c394c000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00001520c372c000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00001520c3528000)
	libstdc++.so.6 => /lib64/libstdc++.so.6 (0x00001520c3193000)
	libm.so.6 => /lib64/libm.so.6 (0x00001520c2e11000)
	libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00001520c2bf9000)
	libc.so.6 => /lib64/libc.so.6 (0x00001520c2834000)
	/lib64/ld-linux-x86-64.so.2 (0x00001520d11f3000)
mpirun -n 2 /home/liuyao/software/Netgauge/netgauge -m mpi -x loggp -o ng_logGP_internode
USAGE: netgauge [global options] -m mode [mode options]

Global options:
  -h, --help                            print help on global (and 
                                        mode-specific) options
  -w, --writemanpage                    create a new manpage based on current 
                                        code and print it to STDOUT
  -i, --init-thread                     initialize with MPI_THREAD_MULTIPLE 
                                        instead of MPI_THREAD_SINGLE
  -v, --verbosity level                 verbose output
  -n, --nooutput                        no output, but benchmark results
  -o, --output=FILENAME                 write output to this file
  -f, --fulltestresults=FILENAME        write all/full testresults to files 
                                        begining with the given filename
  -a, --servermode                      operate in server mode
  -c, --testcount=NUMBER                initial test count
  -t, --time=SECONDS                    maximum time for one test
  -s, --size=BYTE                       data size (range, yy-xx or -xx)
  -g, --gradation=GRADATION             gradation of the geometrical data size 
                                        growth
  -q, --sanity-check                    perform timer sanity check
  --, --hostnames                       print hostnames
  -x, --com_pattern=NAME                communication pattern, defaults to 
                                        "one_one". See list of available 
                                        patterns below.

Mode:
  -m, --mode=NAME                       specifies the mode (required). For 
                                        further information of available modes 
                                        see list below.

Mode mpi options:

Available modes:
  dummy                                 Mode dummy doesn't actually do 
                                        anything.
  tcp                                   Mode tcp uses TCP sockets for data 
                                        transmission.
  udp                                   Mode udp uses UDP sockets for data 
                                        transmission.
  edp                                   Mode edp uses Ethernet Datagram 
                                        Protocol sockets.
  esp                                   Mode esp uses Ethernet Sequenced 
                                        Packets sockets.
  mpi                                   Mode mpi uses MPI_Send/MPI_Recv for 
                                        data transmission.
  ib                                    Mode ib uses Infiniband for data 
                                        transmission
  ibv                                   Mode ibv uses Infiniband 1:1 
                                        communication for data transmission.

Available communication patterns:
  cpu                                   measures cpu performance
  func_args                             measures function call overhead with 
                                        different argument counts
  ebb                                   benchmarks effective bisection 
                                        bandwidth
  mprobe                                benchmarks threaded dynamic receive 
                                        performance
  collvsnoise                           assesses the influence of network 
                                        jitter on collective MPI operations
  synctest                              tests synchronization precision
  noise                                 measures system noise parameters
  loggp                                 measures LogGP Parameters 
                                        (experimental)
  nbov                                  measures overheads for non-blocking 
                                        send and receive
  distrtt                               measures latency distribution
  1toN                                  measures bandwith in a 1:N congestion 
                                        situation
  Nto1                                  measures bandwith in a N:1 congestion 
                                        situation
  one_one_randbuf                       one_one with random buffers
  one_one_randtag                       one_one with random tags and 
                                        MPI_ANY_TAG
  disk                                  measures disk performance
  memory                                measures memory performance
  one_one_dtype                         measures latency&bandwidth with varying 
                                        strides in a datatype
  one_one_req_queue                     measures small-message ping-pong 
                                        latency with varying matching queue 
                                        lengths
  one_one_sync                          measures ping-pong latency&bandwidth 
                                        (MPI_Ssend)
  one_one_perturb                       measures ping-pong latency&bandwidth
  one_one_mpi_bidirect                  measures bidirectional ping-pong 
                                        latency&bandwidth
  one_one_all                           measures ping-pong latency between all 
                                        nodes
  one_one                               measures ping-pong latency&bandwidth
  overlap                               measures overlap ....
USAGE: netgauge [global options] -m mode [mode options]

Global options:
  -h, --help                            print help on global (and 
                                        mode-specific) options
  -w, --writemanpage                    create a new manpage based on current 
                                        code and print it to STDOUT
  -i, --init-thread                     initialize with MPI_THREAD_MULTIPLE 
                                        instead of MPI_THREAD_SINGLE
  -v, --verbosity level                 verbose output
  -n, --nooutput                        no output, but benchmark results
  -o, --output=FILENAME                 write output to this file
  -f, --fulltestresults=FILENAME        write all/full testresults to files 
                                        begining with the given filename
  -a, --servermode                      operate in server mode
  -c, --testcount=NUMBER                initial test count
  -t, --time=SECONDS                    maximum time for one test
  -s, --size=BYTE                       data size (range, yy-xx or -xx)
  -g, --gradation=GRADATION             gradation of the geometrical data size 
                                        growth
  -q, --sanity-check                    perform timer sanity check
  --, --hostnames                       print hostnames
  -x, --com_pattern=NAME                communication pattern, defaults to 
                                        "one_one". See list of available 
                                        patterns below.

Mode:
  -m, --mode=NAME                       specifies the mode (required). For 
                                        further information of available modes 
                                        see list below.

Mode mpi options:

Available modes:
  dummy                                 Mode dummy doesn't actually do 
                                        anything.
  tcp                                   Mode tcp uses TCP sockets for data 
                                        transmission.
  udp                                   Mode udp uses UDP sockets for data 
                                        transmission.
  edp                                   Mode edp uses Ethernet Datagram 
                                        Protocol sockets.
  esp                                   Mode esp uses Ethernet Sequenced 
                                        Packets sockets.
  mpi                                   Mode mpi uses MPI_Send/MPI_Recv for 
                                        data transmission.
  ib                                    Mode ib uses Infiniband for data 
                                        transmission
  ibv                                   Mode ibv uses Infiniband 1:1 
                                        communication for data transmission.

Available communication patterns:
  cpu                                   measures cpu performance
  func_args                             measures function call overhead with 
                                        different argument counts
  ebb                                   benchmarks effective bisection 
                                        bandwidth
  mprobe                                benchmarks threaded dynamic receive 
                                        performance
  collvsnoise                           assesses the influence of network 
                                        jitter on collective MPI operations
  synctest                              tests synchronization precision
  noise                                 measures system noise parameters
  loggp                                 measures LogGP Parameters 
                                        (experimental)
  nbov                                  measures overheads for non-blocking 
                                        send and receive
  distrtt                               measures latency distribution
  1toN                                  measures bandwith in a 1:N congestion 
                                        situation
  Nto1                                  measures bandwith in a N:1 congestion 
                                        situation
  one_one_randbuf                       one_one with random buffers
  one_one_randtag                       one_one with random tags and 
                                        MPI_ANY_TAG
  disk                                  measures disk performance
  memory                                measures memory performance
  one_one_dtype                         measures latency&bandwidth with varying 
                                        strides in a datatype
  one_one_req_queue                     measures small-message ping-pong 
                                        latency with varying matching queue 
                                        lengths
  one_one_sync                          measures ping-pong latency&bandwidth 
                                        (MPI_Ssend)
  one_one_perturb                       measures ping-pong latency&bandwidth
  one_one_mpi_bidirect                  measures bidirectional ping-pong 
                                        latency&bandwidth
  one_one_all                           measures ping-pong latency between all 
                                        nodes
  one_one                               measures ping-pong latency&bandwidth
  overlap                               measures overlap ....
options->mpi_opts->worldrank is:1
options->mpi_opts->worldrank is:0
# Info:   (0): Netgauge v2.4.6 MPI enabled (P=2, threadlevel=0) (/home/liuyao/software/Netgauge/netgauge -m mpi -x loggp -o ng_logGP_internode )
NG_MPI is 1
# initializing x86-64 timer (takes some seconds)
NG_MPI is 1
# Info:   (0): Warming module mpi up ... this may take a while
Testing 1 bytes 100 times:
 L=927.111878  s=1  o_s=1262.025646  o_r=551.245546  g=-nan  G=-nan (-nan GiB/s) lsqu(g,G)=-nan 
Testing 1025 bytes 100 times:
 L=927.111878  s=1025  o_s=1332.669624  o_r=588.872725  g=383.715586  G=0.238208 (0.032797 GiB/s) lsqu(g,G)=-nan 
Testing 2049 bytes 100 times:
 L=927.111878  s=2049  o_s=1188.580428  o_r=1130.311967  g=581.575325  G=-0.339766 (-0.022994 GiB/s) lsqu(g,G)=483.239660 
Testing 3073 bytes 100 times:
 L=927.111878  s=3073  o_s=1126.977084  o_r=1168.287710  g=523.138758  G=-0.254291 (-0.030723 GiB/s) lsqu(g,G)=359.900642 
Testing 4097 bytes 100 times:
 L=927.111878  s=4097  o_s=618.604029  o_r=1461.153566  g=501.556025  G=-0.233234 (-0.033496 GiB/s) lsqu(g,G)=296.482716 
Testing 5121 bytes 100 times:
 L=927.111878  s=5121  o_s=1108.400116  o_r=1255.481635  g=417.948969  G=-0.172044 (-0.045410 GiB/s) lsqu(g,G)=298.042506 
Testing 6145 bytes 100 times:
**** removed value 309.529873 for size 6145 from gresults, lsquares was: 298.042506, new lsquares: 298.042506
 L=927.111878  s=6145  o_s=1285.042079  o_r=546.211019  g=417.948969  G=-0.172044 (-0.045410 GiB/s) lsqu(g,G)=298.042506 
Testing 7169 bytes 100 times:
 L=927.111878  s=7169  o_s=962.086343  o_r=1088.956651  g=317.507703  G=-0.119104 (-0.065594 GiB/s) lsqu(g,G)=302.851383 
Testing 8193 bytes 100 times:
 L=927.111878  s=8193  o_s=1173.864255  o_r=1335.678265  g=221.355241  G=-0.074784 (-0.104468 GiB/s) lsqu(g,G)=329.551789 
Testing 9217 bytes 100 times:
**** removed value 387.356310 for size 9217 from gresults, lsquares was: 329.551789, new lsquares: 329.551789
 L=927.111878  s=9217  o_s=1326.141341  o_r=614.131706  g=221.355241  G=-0.074784 (-0.104468 GiB/s) lsqu(g,G)=329.551789 
Testing 10241 bytes 100 times:
**** removed value 314.558586 for size 10241 from gresults, lsquares was: 329.551789, new lsquares: 329.551789
 L=927.111878  s=10241  o_s=1331.649270  o_r=602.702527  g=221.355241  G=-0.074784 (-0.104468 GiB/s) lsqu(g,G)=329.551789 
Testing 11265 bytes 100 times:
 L=927.111878  s=11265  o_s=1060.920245  o_r=1164.854358  g=162.108718  G=-0.054076 (-0.144472 GiB/s) lsqu(g,G)=317.630186 
Testing 12289 bytes 100 times:
 L=927.111878  s=12289  o_s=1357.329515  o_r=1582.633993  g=91.527569  G=-0.031334 (-0.249326 GiB/s) lsqu(g,G)=330.770955 
Testing 13313 bytes 100 times:
 L=927.111878  s=13313  o_s=1345.911507  o_r=1021.947668  g=21.633654  G=-0.010810 (-0.722683 GiB/s) lsqu(g,G)=354.657679 
Testing 14337 bytes 100 times:
 L=927.111878  s=14337  o_s=1613.646291  o_r=1098.762680  g=-20.997471  G=0.000564 (13.846250 GiB/s) lsqu(g,G)=354.699854 
Testing 15361 bytes 100 times:
 L=927.111878  s=15361  o_s=1290.023460  o_r=682.908929  g=-79.122574  G=0.014681 (0.532154 GiB/s) lsqu(g,G)=374.380416 
Testing 16385 bytes 100 times:
 L=927.111878  s=16385  o_s=1389.213517  o_r=633.204936  g=-107.284944  G=0.020933 (0.373222 GiB/s) lsqu(g,G)=367.239538 
Testing 17409 bytes 100 times:
 L=927.111878  s=17409  o_s=1556.268474  o_r=953.399595  g=-73.066573  G=0.013954 (0.559879 GiB/s) lsqu(g,G)=366.329494 
Testing 18433 bytes 100 times:
 L=927.111878  s=18433  o_s=1250.675572  o_r=644.738971  g=-98.761042  G=0.018794 (0.415687 GiB/s) lsqu(g,G)=360.795499 
Testing 19457 bytes 100 times:
 L=927.111878  s=19457  o_s=1019.421951  o_r=1202.353366  g=-53.098558  G=0.010807 (0.722886 GiB/s) lsqu(g,G)=373.113955 
Testing 20481 bytes 100 times:
 L=927.111878  s=20481  o_s=1372.921932  o_r=987.969604  g=-33.944900  G=0.007682 (1.017054 GiB/s) lsqu(g,G)=365.576820 
Testing 21505 bytes 100 times:
 L=927.111878  s=21505  o_s=1211.919048  o_r=1216.953193  g=-2.373180  G=0.002852 (2.739050 GiB/s) lsqu(g,G)=366.506747 
Testing 22529 bytes 100 times:
 L=927.111878  s=22529  o_s=1258.395159  o_r=544.392047  g=-21.496219  G=0.005605 (1.393833 GiB/s) lsqu(g,G)=360.566592 
Testing 23553 bytes 100 times:
 L=927.111878  s=23553  o_s=1421.258112  o_r=568.177348  g=-40.976146  G=0.008254 (0.946534 GiB/s) lsqu(g,G)=355.583075 
Testing 24577 bytes 100 times:
 L=927.111878  s=24577  o_s=1455.364017  o_r=970.859338  g=-53.614195  G=0.009882 (0.790545 GiB/s) lsqu(g,G)=348.570231 
Testing 25601 bytes 100 times:
 L=927.111878  s=25601  o_s=1297.223849  o_r=678.096277  g=-56.770438  G=0.010269 (0.760780 GiB/s) lsqu(g,G)=340.297235 
Testing 26625 bytes 100 times:
 L=927.111878  s=26625  o_s=1293.185087  o_r=640.032862  g=-52.741865  G=0.009799 (0.797302 GiB/s) lsqu(g,G)=332.687168 
Testing 27649 bytes 100 times:
**** removed value 627.878759 for size 1025 from gresults, lsquares was: 332.687168, new lsquares: 309.922048
 L=927.111878  s=27649  o_s=1172.439412  o_r=1298.994275  g=-134.880808  G=0.012204 (0.640149 GiB/s) lsqu(g,G)=309.922048 
Testing 28673 bytes 100 times:
 L=927.111878  s=28673  o_s=1290.304107  o_r=660.822886  g=-139.823664  G=0.012695 (0.615378 GiB/s) lsqu(g,G)=303.392991 
Testing 29697 bytes 100 times:
 L=927.111878  s=29697  o_s=1325.113859  o_r=641.975448  g=-156.449366  G=0.014285 (0.546910 GiB/s) lsqu(g,G)=300.307981 
Testing 30721 bytes 100 times:
 L=927.111878  s=30721  o_s=1370.499454  o_r=1517.573059  g=-124.836038  G=0.011373 (0.686942 GiB/s) lsqu(g,G)=306.306940 
Testing 31745 bytes 100 times:
 L=927.111878  s=31745  o_s=530.682747  o_r=1279.181514  g=-74.524475  G=0.006901 (1.132154 GiB/s) lsqu(g,G)=329.959743 
Testing 32769 bytes 100 times:
 L=927.111878  s=32769  o_s=614.029364  o_r=1404.437051  g=-34.414385  G=0.003455 (2.261429 GiB/s) lsqu(g,G)=341.911959 
Testing 33793 bytes 100 times:
[node02:3273786:0:3273786] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x6078000)
[node01:3520113:0:3520113] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x52d9000)
netgauge: malloc.c:2396: sysmalloc: Assertion `(old_top == initial_top (av) && old_size == 0) || ((unsigned long) (old_size) >= MINSIZE && prev_inuse (old_top) && ((unsigned long) old_end & (pagesize - 1)) == 0)' failed.
==== backtrace (tid:3520113) ====
 0 0x0000000000012c20 .annobin_sigaction.c()  sigaction.c:0
 1 0x00000000001f3710 cuEGLApiInit()  ???:0
 2 0x00000000002f2d5b cudbgGetAPI()  ???:0
 3 0x00000000003d96e5 cudbgGetAPI()  ???:0
 4 0x0000000000190313 ???()  /lib64/libcuda.so.1:0
 5 0x000000000043d194 cudbgMain()  ???:0
 6 0x0000000000187238 ???()  /lib64/libcuda.so.1:0
 7 0x000000000020bc66 cuTexRefGetArray()  ???:0
 8 0x000000000003dad9 __cudart593()  :0
 9 0x0000000000010d25 __cudart618()  :0
10 0x0000000000060281 cudaMemcpy()  :0
11 0x000000000000a0b1 MycudaMemcpy()  ???:0
12 0x0000000000410345 prtt_do_benchmarks()  /home/liuyao/software/Netgauge/ptrn_loggp.c:347
13 0x0000000000410faa loggp_do_benchmarks()  /home/liuyao/software/Netgauge/ptrn_loggp.c:559
14 0x0000000000404d60 main()  /home/liuyao/software/Netgauge/netgauge.c:351
15 0x0000000000023493 __libc_start_main()  ???:0
16 0x000000000040528e _start()  ???:0
=================================

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 3520113 RUNNING AT node01
=   EXIT CODE: 11
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
[proxy:0:1@node02.cluster] HYD_pmcd_pmip_control_cmd_cb (proxy/pmip_cb.c:480): assert (!closed) failed
[proxy:0:1@node02.cluster] HYDT_dmxu_poll_wait_for_event (lib/tools/demux/demux_poll.c:76): callback returned error status
[proxy:0:1@node02.cluster] main (proxy/pmip.c:127): demux engine error waiting for event
srun: error: node02: task 1: Exited with exit code 7
YOUR APPLICATION TERMINATED WITH THE EXIT STRING: Segmentation fault (signal 11)
This typically refers to a problem with your application.
Please see the FAQ page for debugging suggestions
